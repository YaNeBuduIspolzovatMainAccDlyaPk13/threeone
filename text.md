# –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ: –†–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ Apache Spark –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞ ETL‚Äë–ø–∞–π–ø–ª–∞–π–Ω–∞

## 0. –ö—Ä–∞—Ç–∫–æ: —á—Ç–æ –Ω—É–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å
- –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å –ª–æ–∫–∞–ª—å–Ω—ã–π Spark‚Äë–∫–ª–∞—Å—Ç–µ—Ä (Master + Worker).
- –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ –ø–æ—Å—Ç—Ä–æ–∏—Ç—å ETL‚Äë–ø–∞–π–ø–ª–∞–π–Ω –Ω–∞ PySpark.
- –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏—Ö –∏ –æ—Ñ–æ—Ä–º–∏—Ç—å –æ—Ç—á–µ—Ç.

–§–æ—Ä–º–∞—Ç —Å–¥–∞—á–∏: GitHub‚Äë—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å –∫–æ–¥–æ–º, –æ—Ç—á–µ—Ç–æ–º –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏.  
–°—Ä–æ–∫: 2 –Ω–µ–¥–µ–ª–∏ —Å –º–æ–º–µ–Ω—Ç–∞ –≤—ã–¥–∞—á–∏.  
–î–æ–ø—É—Å—Ç–∏–º–∞—è –ø–æ–º–æ—â—å: –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ—à–µ–Ω–∏–π, –Ω–æ –∫–æ–¥ –∏ –æ—Ç—á–µ—Ç –ø–∏—à–µ—Ç–µ —Å–∞–º–∏.  
–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏: –ü–æ–ª–Ω–æ—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è, –ø–æ–Ω–∏–º–∞–Ω–∏–µ, –∫–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞, –∞–∫–∫—É—Ä–∞—Ç–Ω–æ—Å—Ç—å –æ—Ç—á–µ—Ç–∞.

---

## 1. –¶–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è
–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤—ã —Å–º–æ–∂–µ—Ç–µ:
1. –û–±—ä—è—Å–Ω–∏—Ç—å –±–∞–∑–æ–≤—É—é –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Spark –∏ —Ä–æ–ª—å Driver, Master, Worker, Executor.
2. –†–∞–∑–≤–µ—Ä–Ω—É—Ç—å standalone‚Äë–∫–ª–∞—Å—Ç–µ—Ä Spark –Ω–∞ Windows.
3. –ü–æ—Å—Ç—Ä–æ–∏—Ç—å ETL‚Äë–ø—Ä–æ—Ü–µ—Å—Å (Extract ‚Üí Transform ‚Üí Load) –≤ PySpark.
4. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –æ—Ñ–æ—Ä–º–∏—Ç—å –æ—Ç—á–µ—Ç.

---

## 2. –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∏ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
- –û–°: Windows 10/11.
- Python: 3.9‚Äì3.11.
- JDK: 11 (LTS).
- Apache Spark: 3.5.1 (Hadoop 3).
- –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã: PowerShell, VS Code, Git.

---

## 3. –¢–µ–æ—Ä–∏—è –≤ 5 –∞–±–∑–∞—Ü–∞—Ö (–º–∏–Ω–∏–º—É–º, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞–¥–æ –∑–Ω–∞—Ç—å)
Apache Spark ‚Äî —Ñ—Ä–µ–π–º–≤–æ—Ä–∫ –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –Ω–∞ in‚Äëmemory –æ–±—Ä–∞–±–æ—Ç–∫—É (–±—ã—Å—Ç—Ä–µ–µ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ MapReduce).

–ö–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- Driver Program ‚Äî –≤–∞—à–∞ –ø—Ä–æ–≥—Ä–∞–º–º–∞, –∫–æ—Ç–æ—Ä–∞—è —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç –∑–∞–¥–∞–Ω–∏—è.
- Cluster Manager ‚Äî —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ—Å—É—Ä—Å—ã (–≤ –∑–∞–¥–∞–Ω–∏–∏ ‚Äî standalone).
- Master ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞.
- Worker ‚Äî —Ä–∞–±–æ—á–∏–π —É–∑–µ–ª, –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á–∏.
- Executor ‚Äî –ø—Ä–æ—Ü–µ—Å—Å –Ω–∞ Worker, –≤—ã–ø–æ–ª–Ω—è—é—â–∏–π –≤—ã—á–∏—Å–ª–µ–Ω–∏—è.

---

## 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ä–µ–¥—ã —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏

### 4.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ JDK 11
Spark —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ JVM, –ø–æ—ç—Ç–æ–º—É Java –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞.

–®–∞–≥–∏:
1. –°–∫–∞—á–∞–π—Ç–µ JDK 11: https://adoptium.net/temurin/releases/
2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –ø—É—Ç—å –≤–∏–¥–∞: C:\Program Files\Eclipse Adoptium\jdk-11.x.x.x).
3. –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤ PowerShell:
        java -version
    
    –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: openjdk version "11.0.xx".

–¢–∏–ø–æ–≤—ã–µ –æ—à–∏–±–∫–∏:

| –û—à–∏–±–∫–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|--------|---------|---------|
| 'java' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π –∏–ª–∏ –≤–Ω–µ—à–Ω–µ–π –∫–æ–º–∞–Ω–¥–æ–π | PATH –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω | –î–æ–±–∞–≤–∏—Ç—å %JAVA_HOME%\bin –≤ PATH |
| –û—à–∏–±–∫–∞ jre1.8.x –≤ –ø—É—Ç–∏ | –ö–æ–Ω—Ñ–ª–∏–∫—Ç —Å—Ç–∞—Ä—ã—Ö Java | –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ –≤–µ—Ä—Å–∏–∏ Java |
| The system cannot find the path specified | –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å | –ü–µ—Ä–µ—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—É—Ç—å |

### 4.2 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Apache Spark
–®–∞–≥–∏:
1. –°–∫–∞—á–∞–π—Ç–µ Spark 3.5.1 (Pre‚Äëbuilt for Hadoop 3): https://spark.apache.org/downloads.html
2. –†–∞—Å–ø–∞–∫—É–π—Ç–µ –≤ C:\spark\spark-3.5.1-bin-hadoop3.
3. –ü—Ä–æ–≤–µ—Ä–∫–∞:
        dir C:\spark\
    

### 4.3 –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
–ù—É–∂–Ω—ã —Ç—Ä–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ: JAVA_HOME, SPARK_HOME, PATH.

–®–∞–≥–∏:
1. –û—Ç–∫—Ä–æ–π—Ç–µ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ä–µ–¥—ã Windows.
2. –°–æ–∑–¥–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:
    - JAVA_HOME = C:\Program Files\Eclipse Adoptium\jdk-11.x.x.x
    - SPARK_HOME = C:\spark\spark-3.5.1-bin-hadoop3
3. –í Path –¥–æ–±–∞–≤—å—Ç–µ:
    
    %JAVA_HOME%\bin
    %SPARK_HOME%\bin
    
4. –ü—Ä–æ–≤–µ—Ä–∫–∞:
        echo "JAVA_HOME: $env:JAVA_HOME"
    echo "SPARK_HOME: $env:SPARK_HOME"
    java -version
    spark-shell --version 2>&1 | Select-String "version"
    

---

## 5. –ó–∞–ø—É—Å–∫ Spark‚Äë–∫–ª–∞—Å—Ç–µ—Ä–∞ (standalone)

### 5.1 –ó–∞–ø—É—Å–∫ Master
cd $env:SPARK_HOME
mkdir logs
\bin\spark-class.cmd org.apache.spark.deploy.master.Master
–í –≤—ã–≤–æ–¥–µ –Ω–∞–π–¥–∏—Ç–µ:
SparkUI available at http://–í–ê–®_IP:8080
Master spark://–í–ê–®_–ö–û–ú–ü–¨–Æ–¢–ï–†:7077
–°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –æ–±–∞ –∞–¥—Ä–µ—Å–∞.

### 5.2 –ó–∞–ø—É—Å–∫ Worker
–í –Ω–æ–≤–æ–º PowerShell:
cd $env:SPARK_HOME
\bin\spark-class.cmd org.apache.spark.deploy.worker.Worker spark://–í–ê–®_–ö–û–ú–ü–¨–Æ–¢–ï–†:7077

–ü—Ä–æ–≤–µ—Ä–∫–∞:
- –í Spark UI (http://localhost:8080) –¥–æ–ª–∂–µ–Ω –ø–æ—è–≤–∏—Ç—å—Å—è Worker.

–¢–∏–ø–æ–≤—ã–µ –æ—à–∏–±–∫–∏:

| –û—à–∏–±–∫–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|--------|---------|---------|
| Failed to connect to master | Master –Ω–µ –∑–∞–ø—É—â–µ–Ω | –ó–∞–ø—É—Å—Ç–∏—Ç—å Master –ø–µ—Ä–≤—ã–º |
| Address already in use | –ü–æ—Ä—Ç –∑–∞–Ω—è—Ç | –û—Å–≤–æ–±–æ–¥–∏—Ç—å –ø–æ—Ä—Ç |
| Connection refused | –ë—Ä–∞–Ω–¥–º–∞—É—ç—Ä | –î–æ–±–∞–≤–∏—Ç—å –∏—Å–∫–ª—é—á–µ–Ω–∏–µ |

### 5.3 –°–∫—Ä–∏–ø—Ç—ã –¥–ª—è –∑–∞–ø—É—Å–∫–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
start_master.ps1
Start-Process powershell -Verb RunAs -ArgumentList @"
cd $env:SPARK_HOME
\bin\spark-class.cmd org.apache.spark.deploy.master.Master
"@
start_worker.ps1
$master_url = "spark://–í–ê–®_–ö–û–ú–ü–¨–Æ–¢–ï–†:7077"
Start-Process powershell -Verb RunAs -ArgumentList @"
cd $env:SPARK_HOME
\bin\spark-class.cmd org.apache.spark.deploy.worker.Worker $master_url
"@

---

## 6. –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ ETL‚Äë–ø–∞–π–ø–ª–∞–π–Ω–∞

### 6.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
spark_etl_project/
‚îú‚îÄ‚îÄ data/
‚îú‚îÄ‚îÄ src/
‚îú‚îÄ‚îÄ notebooks/
‚îú‚îÄ‚îÄ output/
‚îú‚îÄ‚îÄ docs/
‚îî‚îÄ‚îÄ README.md

### 6.2 –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
cd spark_etl_project
python -m venv venv
\venv\Scripts\activate
pip install pyspark==3.5.1 pandas==2.0.3 faker==20.1.0 openpyxl==3.1.2

requirements.txt
pyspark==3.5.1
pandas==2.0.3
faker==20.1.0
openpyxl==3.1.2

### 6.3 –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
src/generate_data.py
"""
–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞.
–°–æ–∑–¥–∞–µ—Ç CSV-—Ñ–∞–π–ª —Å 50 000 –∑–∞–ø–∏—Å–µ–π –æ –¥–µ–π—Å—Ç–≤–∏—è—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π.
"""
import pandas as pd
import numpy as np
from faker import Faker
from datetime import datetime, timedelta
import random
import os

def generate_clickstream_data(num_records=50000, output_path="../data/clickstream.csv"):
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–∞–Ω–Ω—ã–µ –æ –∫–ª–∏–∫–∞—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    
    Args:
        num_records: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π
        output_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞
    """
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    fake = Faker('ru_RU')
    np.random.seed(42)
    random.seed(42)
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö...")
    
    # –°–ø–∏—Å–∫–∏ –≤–æ–∑–º–æ–∂–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
    actions = ['click', 'view', 'purchase', 'login', 'logout', 'search', 'add_to_cart']
    devices = ['mobile', 'desktop', 'tablet']
    regions = ['–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥', 
               '–ö–∞–∑–∞–Ω—å', '–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥', '–ß–µ–ª—è–±–∏–Ω—Å–∫', '–°–∞–º–∞—Ä–∞']
    
    data = []
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è "—Ö–æ—Ä–æ—à–∏—Ö" –¥–∞–Ω–Ω—ã—Ö
    for i in range(num_records):
        if i % 10000 == 0:
            print(f"  –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {i} –∑–∞–ø–∏—Å–µ–π...")
            
        record = {
            'user_id': fake.uuid4()[:8],
            'session_id': f"sess_{fake.random_number(digits=8)}",
            'action': random.choice(actions),
            'timestamp': (datetime.now() - timedelta(
                days=random.randint(0, 30),
                hours=random.randint(0, 23),
                minutes=random.randint(0, 59)
            )).strftime('%Y-%m-%d %H:%M:%S'),
            'region': random.choice(regions),
            'device': random.choice(devices),
            'duration_sec': random.randint(1, 600),
            'product_id': f"prod_{random.randint(1000, 9999)}",
            'price': round(random.uniform(10, 1000), 2)
        }
        data.append(record)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ "–ø–ª–æ—Ö–∏—Ö" –¥–∞–Ω–Ω—ã—Ö (10% –æ—Ç –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞)
    bad_records = num_records // 10
    print(f"–î–æ–±–∞–≤–ª—è–µ–º {bad_records} '–ø–ª–æ—Ö–∏—Ö' –∑–∞–ø–∏—Å–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ—á–∏—Å—Ç–∫–∏...")
    
    for i in range(bad_records):
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å –ø—Ä–æ–±–ª–µ–º–∞–º–∏
        problems = [
            {'user_id': ''},  # –ü—É—Å—Ç–æ–π ID
            {'session_id': None},  # –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            {'timestamp': '2024-13-45 25:61:61'},  # –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –¥–∞—Ç–∞
            {'duration_sec': -random.randint(1, 100)},  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è
            {'device': 'smart_watch'},  # –ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            {'price': -random.uniform(10, 100)},  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è —Ü–µ–Ω–∞
            {'action': 'unknown_action'},  # –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ
            {'region': ''}  # –ü—É—Å—Ç–æ–π —Ä–µ–≥–∏–æ–Ω
        ]
        
        base_record = {
            'user_id': fake.uuid4()[:8],
            'session_id': f"sess_{fake.random_number(digits=8)}",
            'action': random.choice(actions),
            'timestamp': (datetime.now() - timedelta(days=random.randint(0, 30))).strftime('%Y-%m-%d %H:%M:%S'),
            'region': random.choice(regions),
            'device': random.choice(devices),
            'duration_sec': random.randint(1, 600),
            'product_id': f"prod_{random.randint(1000, 9999)}",
            'price': round(random.uniform(10, 1000), 2)
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É
        problem = random.choice(problems)
        base_record.update(problem)
        data.append(base_record)
    
    # –°–æ–∑–¥–∞–µ–º DataFrame

    df = pd.DataFrame(data)
    
    # –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    print("\n" + "="*60)
    print("‚úÖ –î–ê–ù–ù–´–ï –£–°–ü–ï–®–ù–û –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–´")
    print("="*60)
    print(f"üìä –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {len(df):,}")
    print(f"üìÅ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
    print("\nüìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"  - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {df['user_id'].nunique()}")
    print(f"  - –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤: {df['region'].nunique()}")
    print(f"  - –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {df['timestamp'].min()} - {df['timestamp'].max()}")
    print(f"  - –ü—É—Å—Ç—ã—Ö user_id: {df['user_id'].isna().sum() + (df['user_id'] == '').sum()}")
    print(f"  - –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö duration_sec: {(df['duration_sec'] < 0).sum()}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
    print("\nüëÄ –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–ø–µ—Ä–≤—ã–µ 3 —Å—Ç—Ä–æ–∫–∏):")
    print(df.head(3).to_string())
    
    return df

if __name__ == "__main__":
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
    df = generate_clickstream_data(50000)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print(df.info())

–ó–∞–ø—É—Å—Ç–∏—Ç–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä:
cd src
python generate_data.py

–ü—Ä–æ–≤–µ—Ä–∫–∞:
- –í –ø–∞–ø–∫–µ data/ –¥–æ–ª–∂–µ–Ω –ø–æ—è–≤–∏—Ç—å—Å—è —Ñ–∞–π–ª clickstream.csv
- –§–∞–π–ª –¥–æ–ª–∂–µ–Ω –æ—Ç–∫—Ä—ã–≤–∞—Ç—å—Å—è –≤ Excel
- –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞ –ø—Ä–∏–º–µ—Ä–Ω–æ 10-15 MB

–¢–∏–ø–æ–≤—ã–µ –æ—à–∏–±–∫–∏:

| –û—à–∏–±–∫–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|--------|---------|---------|
| No module named 'faker' | –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ | pip install faker |
| PermissionError | –ù–µ—Ç –ø—Ä–∞–≤ –Ω–∞ –∑–∞–ø–∏—Å—å | –ó–∞–ø—É—Å—Ç–∏—Ç—å VS Code –æ—Ç –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–∞ |
| MemoryError | –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ | –£–º–µ–Ω—å—à–∏—Ç—å num_records –¥–æ 10000 |

### 6.4 –°–æ–∑–¥–∞–Ω–∏–µ ETL‚Äë–ø–∞–π–ø–ª–∞–π–Ω–∞

`src/etl_pipeline.py`:
"""
–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π ETL-–ø–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∏–∫–æ–≤
–í—ã–ø–æ–ª–Ω—è–µ—Ç: –æ—á–∏—Å—Ç–∫—É, —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—é, –∞–≥—Ä–µ–≥–∞—Ü–∏—é –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
"""
import os
import sys
from datetime import datetime
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import (
    StructType, StructField, StringType, 
    TimestampType, IntegerType, DoubleType, DateType
)
from pyspark.sql.window import Window


class SparkETLPipeline:
    """–ö–ª–∞—Å—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è ETL-–ø–∞–π–ø–ª–∞–π–Ω–æ–º Spark"""
    
    def __init__(self, master_url="spark://localhost:7077", app_name="ETL_Pipeline"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Spark —Å–µ—Å—Å–∏–∏
        
        Args:
            master_url: URL Spark Master
            app_name: –ò–º—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        """
        self.start_time = datetime.now()
        
        try:
            self.spark = SparkSession.builder \
                .appName(app_name) \
                .master(master_url) \
                .config("spark.sql.shuffle.partitions", "8") \
                .config("spark.executor.memory", "2g") \
                .config("spark.driver.memory", "2g") \
                .config("spark.sql.adaptive.enabled", "true") \
                .getOrCreate()
            
            self.log("="*60)
            self.log(f"üöÄ –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø SPARK –°–ï–°–°–ò–ò")
            self.log(f"   –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: {app_name}")
            self.log(f"   Master URL: {master_url}")
            self.log(f"   –í–µ—Ä—Å–∏—è Spark: {self.spark.version}")
            self.log("="*60)
            
        except Exception as e:
            self.log(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ Spark —Å–µ—Å—Å–∏–∏: {e}", level="ERROR")
            sys.exit(1)
    
    def log(self, message, level="INFO"):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"[{timestamp}] [{level}] {message}")
    
    def extract(self, file_path):
        """
        –≠—Ç–∞–ø EXTRACT: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ CSV
        
        Args:
            file_path: –ü—É—Ç—å –∫ CSV —Ñ–∞–π–ª—É
            
        Returns:
            DataFrame —Å –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        self.log("üì• –≠—Ç–∞–ø EXTRACT: –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å—Ö–µ–º—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Ç–∏–ø–æ–≤
        schema = StructType([
            StructField("user_id", StringType(), True),
            StructField("session_id", StringType(), True),
            StructField("action", StringType(), True),
            StructField("timestamp", StringType(), True),
            StructField("region", StringType(), True),
            StructField("device", StringType(), True),
            StructField("duration_sec", IntegerType(), True),
            StructField("product_id", StringType(), True),
            StructField("price", DoubleType(), True)
        ])
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫
            raw_df = self.spark.read \
                .option("header", "true") \
                .option("encoding", "utf-8-sig") \
                .option("mode", "PERMISSIVE") \
                .option("columnNameOfCorruptRecord", "_corrupt_record") \
                .schema(schema) \
                .csv(file_path)
            
            initial_count = raw_df.count()
            corrupt_count = raw_df.filter(F.col("_corrupt_record").isNotNull()).count()
            
            self.log(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {initial_count:,}")
            self.log(f"   –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {corrupt_count}")
            
            if corrupt_count > 0:
                self.log(f"   ‚ö†Ô∏è  –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏", level="WARN")
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                corrupt_df = raw_df.filter(F.col("_corrupt_record").isNotNull())
                corrupt_path = "../output/corrupt_records"
                corrupt_df.select("_corrupt_record") \
                    .write \
                    .mode("overwrite") \
                    .csv(corrupt_path)
                self.log(f"   –ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {corrupt_path}")
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ –∑–∞–ø–∏—Å–∏
            clean_df = raw_df.filter(F.col("_corrupt_record").isNull()) \
                .drop("_corrupt_record")
            
            return clean_df
            
        except Exception as e:
            self.log(f"‚ùå –û–®–ò–ë–ö–ê –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}", level="ERROR")
            raise
    
    def transform(self, df):
        """
        –≠—Ç–∞–ø TRANSFORM: –û—á–∏—Å—Ç–∫–∞ –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        
        Args:

            df: –í—Ö–æ–¥–Ω–æ–π DataFrame
            
        Returns:
            –û—á–∏—â–µ–Ω–Ω—ã–π –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–Ω—ã–π DataFrame
        """
        self.log("üîß –≠—Ç–∞–ø TRANSFORM: –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
        
        # 1. –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞
        self.log("   1. –ë–∞–∑–æ–≤–∞—è –æ—á–∏—Å—Ç–∫–∞...")
        cleaned_df = df \
            .dropDuplicates(["user_id", "session_id", "timestamp"]) \
            .fillna({
                "region": "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "device": "unknown",
                "user_id": "unknown_user"
            }) \
            .filter(F.col("user_id") != "") \
            .filter(F.col("duration_sec") > 0) \
            .filter(F.col("price") >= 0)
        
        # 2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–µ–π
        self.log("   2. –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤...")
        transformed_df = cleaned_df \
            .withColumn("event_timestamp", F.to_timestamp("timestamp", "yyyy-MM-dd HH:mm:ss")) \
            .withColumn("event_date", F.to_date("event_timestamp")) \
            .withColumn("event_hour", F.hour("event_timestamp")) \
            .withColumn("event_dayofweek", F.dayofweek("event_timestamp")) \
            .withColumn("session_category",
                       F.when(F.col("duration_sec") < 60, "short")
                        .when(F.col("duration_sec") <= 300, "medium")
                        .otherwise("long")) \
            .withColumn("price_category",
                       F.when(F.col("price") < 100, "low")
                        .when(F.col("price") <= 500, "medium")
                        .otherwise("high"))
        
        # 3. –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã—Ö –¥–∞—Ç
        transformed_df = transformed_df.filter(F.col("event_timestamp").isNotNull())
        
        self.log(f"   –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {transformed_df.count():,} –∑–∞–ø–∏—Å–µ–π")
        
        return transformed_df
    
    def analyze(self, df):
        """
        –≠—Ç–∞–ø ANALYZE: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            df: –û—á–∏—â–µ–Ω–Ω—ã–π DataFrame
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ DataFrame
        """
        self.log("üìä –≠—Ç–∞–ø ANALYZE: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö...")
        
        results = {}
        
        # 1. –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –∏ —á–∞—Å–∞–º
        self.log("   1. –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º –∏ –≤—Ä–µ–º–µ–Ω–∏...")
        results["activity_by_region_hour"] = df.groupBy("region", "event_date", "event_hour") \
            .agg(
                F.count("*").alias("total_events"),
                F.countDistinct("user_id").alias("unique_users"),
                F.avg("duration_sec").alias("avg_duration"),
                F.sum("price").alias("total_revenue"),
                F.avg("price").alias("avg_price")
            ) \
            .orderBy("region", "event_date", "event_hour")
        
        # 2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º
        self.log("   2. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞–º...")
        results["device_statistics"] = df.groupBy("device", "session_category") \
            .agg(
                F.count("*").alias("session_count"),
                F.avg("duration_sec").alias("avg_duration"),
                F.countDistinct("user_id").alias("unique_users")
            ) \
            .orderBy("device", F.col("session_count").desc())
        
        # 3. –¢–æ–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –ø–æ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        self.log("   3. –¢–æ–ø –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π...")
        window_spec = Window.orderBy(F.col("total_sessions").desc())
        results["top_users"] = df.groupBy("user_id", "region") \
            .agg(
                F.count("*").alias("total_sessions"),
                F.sum("duration_sec").alias("total_time"),
                F.sum("price").alias("total_spent")
            ) \
            .withColumn("rank", F.row_number().over(window_spec)) \
            .filter(F.col("rank") <= 100) \
            .orderBy("rank")
        
        # 4. –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
        self.log("   4. –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å...")
        results["daily_activity"] = df.groupBy("event_date") \
            .agg(
                F.count("*").alias("daily_events"),
                F.countDistinct("user_id").alias("daily_users"),

                F.sum("price").alias("daily_revenue")
            ) \
            .orderBy("event_date")
        
        return results
    
    def load(self, cleaned_df, results_dict):
        """
        –≠—Ç–∞–ø LOAD: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            cleaned_df: –û—á–∏—â–µ–Ω–Ω—ã–π DataFrame
            results_dict: –°–ª–æ–≤–∞—Ä—å —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        self.log("üíæ –≠—Ç–∞–ø LOAD: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        base_path = "../output"
        os.makedirs(f"{base_path}/cleaned_data", exist_ok=True)
        os.makedirs(f"{base_path}/aggregated", exist_ok=True)
        os.makedirs(f"{base_path}/reports", exist_ok=True)
        
        # 1. –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ Parquet (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç)
        self.log("   1. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        cleaned_df.write \
            .mode("overwrite") \
            .partitionBy("event_date") \
            .parquet(f"{base_path}/cleaned_data/clickstream_cleaned")
        
        # 2. –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        self.log("   2. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
        for name, df in results_dict.items():
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            df.write \
                .mode("overwrite") \
                .parquet(f"{base_path}/aggregated/{name}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ CSV –¥–ª—è —É–¥–æ–±—Å—Ç–≤–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
            df.coalesce(1) \
                .write \
                .mode("overwrite") \
                .option("header", "true") \
                .option("delimiter", ";") \
                .csv(f"{base_path}/reports/{name}_report")
        
        # 3. –°–æ–∑–¥–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
        self.create_report(cleaned_df, results_dict, base_path)
        
        self.log(f"   üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {base_path}/")
    
    def create_report(self, cleaned_df, results_dict, base_path):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞ –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏"""
        report_path = f"{base_path}/execution_report.txt"
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write("="*60 + "\n")
            f.write("–û–¢–ß–ï–¢ –û –í–´–ü–û–õ–ù–ï–ù–ò–ò ETL-–ü–ê–ô–ü–õ–ê–ô–ù–ê\n")
            f.write("="*60 + "\n\n")
            
            f.write(f"–î–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {datetime.now()}\n")
            f.write(f"–ò–º—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è: {self.spark.conf.get('spark.app.name')}\n")
            f.write(f"–í–µ—Ä—Å–∏—è Spark: {self.spark.version}\n\n")
            
            f.write("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ê–ù–ù–´–•:\n")
            f.write("-"*40 + "\n")
            f.write(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: {cleaned_df.count():,}\n")
            f.write(f"–ö–æ–ª–æ–Ω–æ–∫ –≤ –¥–∞–Ω–Ω—ã—Ö: {len(cleaned_df.columns)}\n")
            f.write(f"–î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: {cleaned_df.agg(F.min('event_date')).collect()[0][0]} "
                   f"- {cleaned_df.agg(F.max('event_date')).collect()[0][0]}\n")
            f.write(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {cleaned_df.select('user_id').distinct().count():,}\n")
            f.write(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤: {cleaned_df.select('region').distinct().count()}\n\n")
            
            f.write("–°–û–•–†–ê–ù–ï–ù–ù–´–ï –§–ê–ô–õ–´:\n")
            f.write("-"*40 + "\n")
            f.write("1. –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: output/cleaned_data/\n")
            f.write("2. –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: output/aggregated/\n")
            f.write("3. –û—Ç—á–µ—Ç—ã –≤ CSV: output/reports/\n")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            f.write("\n–ü–†–ò–ú–ï–†–´ –î–ê–ù–ù–´–•:\n")
            f.write("-"*40 + "\n")
            
            # –ü—Ä–∏–º–µ—Ä –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            f.write("–û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ 5 –∑–∞–ø–∏—Å–µ–π):\n")
            sample_data = cleaned_df.limit(5).collect()
            for row in sample_data:
                f.write(str(row) + "\n")
            
            # –ü—Ä–∏–º–µ—Ä –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            f.write("\n–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ 3 –∑–∞–ø–∏—Å–∏):\n")
            first_key = list(results_dict.keys())[0]
            sample_agg = results_dict[first_key].limit(3).collect()
            for row in sample_agg:
                f.write(str(row) + "\n")

        self.log(f"   üìÑ –û—Ç—á–µ—Ç —Å–æ–∑–¥–∞–Ω: {report_path}")
    
    def run(self, input_path):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞"""
        try:
            # EXTRACT
            raw_data = self.extract(input_path)
            
            # TRANSFORM
            cleaned_data = self.transform(raw_data)
            
            # ANALYZE
            analysis_results = self.analyze(cleaned_data)
            
            # LOAD
            self.load(cleaned_data, analysis_results)
            
            # –í—ã–≤–æ–¥ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            execution_time = datetime.now() - self.start_time
            self.log("="*60)
            self.log(f"‚úÖ ETL-–ü–ê–ô–ü–õ–ê–ô–ù –£–°–ü–ï–®–ù–û –í–´–ü–û–õ–ù–ï–ù")
            self.log(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {execution_time}")
            self.log("="*60)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            self.show_samples(cleaned_data, analysis_results)
            
            return True
            
        except Exception as e:
            self.log(f"‚ùå –û–®–ò–ë–ö–ê –í –ü–ê–ô–ü–õ–ê–ô–ù–ï: {e}", level="ERROR")
            import traceback
            traceback.print_exc()
            return False
    
    def show_samples(self, cleaned_df, results_dict):
        """–ü–æ–∫–∞–∑ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        print("\n" + "="*60)
        print("–û–ë–†–ê–ó–¶–´ –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
        print("="*60)
        
        print("\n1. –û–ß–ò–©–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï (–ø–µ—Ä–≤—ã–µ 5 —Å—Ç—Ä–æ–∫):")
        cleaned_df.select("user_id", "action", "region", "device", "duration_sec", "event_date") \
            .show(5, truncate=False)
        
        print("\n2. –ê–ö–¢–ò–í–ù–û–°–¢–¨ –ü–û –†–ï–ì–ò–û–ù–ê–ú (—Ç–æ–ø-5):")
        results_dict["activity_by_region_hour"] \
            .groupBy("region") \
            .agg(F.sum("total_events").alias("total_events")) \
            .orderBy(F.col("total_events").desc()) \
            .show(5, truncate=False)
        
        print("\n3. –ï–ñ–ï–î–ù–ï–í–ù–ê–Ø –ê–ö–¢–ò–í–ù–û–°–¢–¨ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–Ω–µ–π):")
        results_dict["daily_activity"] \
            .orderBy(F.col("event_date").desc()) \
            .show(5, truncate=False)
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark —Å–µ—Å—Å–∏–∏"""
        self.log("–û—Å—Ç–∞–Ω–æ–≤–∫–∞ Spark —Å–µ—Å—Å–∏–∏...")
        self.spark.stop()

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    INPUT_FILE = "../data/clickstream.csv"
    MASTER_URL = "spark://localhost:7077"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ —Å–≤–æ–π –∞–¥—Ä–µ—Å Master
    
    # –°–æ–∑–¥–∞–µ–º –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = SparkETLPipeline(
        master_url=MASTER_URL,
        app_name="Student_ETL_Pipeline"
    )
    
    try:
        success = pipeline.run(INPUT_FILE)
        if success:
            print("\nüéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º! ETL-–ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω!")
            print("üìã –ß—Ç–æ –¥–µ–ª–∞—Ç—å –¥–∞–ª—å—à–µ:")
            print("   1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–∞–ø–∫—É output/")
            print("   2. –û—Ç–∫—Ä–æ–π—Ç–µ Web UI Spark: http://localhost:8080")
            print("   3. –°–¥–µ–ª–∞–π—Ç–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –¥–ª—è –æ—Ç—á–µ—Ç–∞")
        else:
            print("\nüí• –í –ø—Ä–æ—Ü–µ—Å—Å–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤–æ–∑–Ω–∏–∫–ª–∏ –æ—à–∏–±–∫–∏")
            print("   –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—ã–≤–æ–¥ –≤—ã—à–µ –∏ –∏—Å–ø—Ä–∞–≤—å—Ç–µ –æ—à–∏–±–∫–∏")
            
    finally:
        pipeline.stop()

if __name__ == "__main__":
    main()

–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞:
cd src
python etl_pipeline.py

–¢–∏–ø–æ–≤—ã–µ –æ—à–∏–±–∫–∏:

| –û—à–∏–±–∫–∞ | –ü—Ä–∏—á–∏–Ω–∞ | –†–µ—à–µ–Ω–∏–µ |
|--------|---------|---------|
| Py4JNetworkError | –ù–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Spark Master | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∑–∞–ø—É—â–µ–Ω –ª–∏ Master –∏ Worker |
| java.lang.OutOfMemoryError | –ù–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ | –£–≤–µ–ª–∏—á–∏—Ç—å spark.executor.memory |
| AnalysisException | –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å SQL | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏–º–µ–Ω–∞ –∫–æ–ª–æ–Ω–æ–∫ |
| FileNotFoundException | –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω | –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É |

### 6.5 –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
–°–æ–∑–¥–∞–π—Ç–µ —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏:
`src/verify_results.py`:
import os
from pyspark.sql import SparkSession

def verify_results():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è ETL"""
    
    spark = SparkSession.builder \
        .appName("VerifyResults") \
        .master("local[*]") \
        .getOrCreate()
    
    print("üîç –ü–†–û–í–ï–†–ö–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í ETL")
    print("="*60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ñ–∞–π–ª–æ–≤
    base_path = "../output"
    
    if not os.path.exists(base_path):
        print("‚ùå –ü–∞–ø–∫–∞ output –Ω–µ –Ω–∞–π–¥–µ–Ω–∞!")
        return
    
    # 1. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    parquet_path = f"{base_path}/cleaned_data/clickstream_cleaned"
    if os.path.exists(parquet_path):
        print("\n1. –û–ß–ò–©–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
        df = spark.read.parquet(parquet_path)
        print(f"   –ó–∞–ø–∏—Å–µ–π: {df.count():,}")
        print(f"   –ö–æ–ª–æ–Ω–æ–∫: {len(df.columns)}")
        print("   –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
        df.select("user_id", "action", "event_date").show(3, truncate=False)
    else:
        print("‚ùå –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    aggregated_path = f"{base_path}/aggregated"
    if os.path.exists(aggregated_path):
        print("\n2. –ê–ì–†–ï–ì–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï:")
        for folder in os.listdir(aggregated_path):
            folder_path = os.path.join(aggregated_path, folder)
            if os.path.isdir(folder_path):
                df = spark.read.parquet(folder_path)
                print(f"   {folder}: {df.count()} –∑–∞–ø–∏—Å–µ–π")
    else:
        print("‚ùå –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Ç—á–µ—Ç—ã
    reports_path = f"{base_path}/reports"
    if os.path.exists(reports_path):
        print("\n3. –û–¢–ß–ï–¢–´:")
        for folder in os.listdir(reports_path):
            folder_path = os.path.join(reports_path, folder)
            if os.path.isdir(folder_path):
                # –ò—â–µ–º CSV —Ñ–∞–π–ª—ã
                for file in os.listdir(folder_path):
                    if file.endswith('.csv'):
                        csv_path = os.path.join(folder_path, file)
                        df = spark.read.csv(csv_path, header=True, sep=";")
                        print(f"   {file}: {df.count()} –∑–∞–ø–∏—Å–µ–π")
    else:
        print("‚ùå –û—Ç—á–µ—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    # 4. –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç
    report_file = f"{base_path}/execution_report.txt"
    if os.path.exists(report_file):
        print("\n4. –¢–ï–ö–°–¢–û–í–´–ô –û–¢–ß–ï–¢:")
        with open(report_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            for line in lines[:10]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 10 —Å—Ç—Ä–æ–∫
                print(f"   {line.strip()}")
    else:
        print("‚ùå –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    spark.stop()
    
    print("\n" + "="*60)
    print("‚úÖ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*60)

if __name__ == "__main__":
    verify_results()

---

## 7. –û—Ñ–æ—Ä–º–ª–µ–Ω–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏ –æ—Ç—á–µ—Ç–∞

### 7.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è GitHub
student_spark_project/
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ clickstream.csv
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ generate_data.py
‚îÇ   ‚îú‚îÄ‚îÄ etl_pipeline.py
‚îÇ   ‚îî‚îÄ‚îÄ verify_results.py
‚îú‚îÄ‚îÄ output/ (–Ω–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å –≤ GitHub)
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îú‚îÄ‚îÄ screenshots/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ spark_ui.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ terminal_output.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results_folder.png
‚îÇ   ‚îî‚îÄ‚îÄ report.md
‚îî‚îÄ‚îÄ notebooks/ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    ‚îî‚îÄ‚îÄ analysis.ipynb

.gitignore (–º–∏–Ω–∏–º—É–º):
venv/
__pycache__/
.ipynb_checkpoints/
output/
*.parquet
*.log

### 7.2 –§–∞–π–ª README.md
# –ü—Ä–æ–µ–∫—Ç: ETL-–ø–∞–π–ø–ª–∞–π–Ω –Ω–∞ Apache Spark

## –û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞
–ü—Ä–æ–µ–∫—Ç –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–æ–∑–¥–∞–Ω–∏–µ ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Apache Spark.

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
- `data/` - –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
- `src/` - –∏—Å—Ö–æ–¥–Ω—ã–π –∫–æ–¥
- `docs/` - –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
- `notebooks/` - Jupyter notebooks –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è
- Python 3.9‚Äì3.11
- JDK 11
- Apache Spark 3.5.1

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞
1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```bash
pip install -r requirements.txt

2. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è (—Å–º. –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é)

## –ó–∞–ø—É—Å–∫
1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Spark –∫–ª–∞—Å—Ç–µ—Ä
2. –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π—Ç–µ –¥–∞–Ω–Ω—ã–µ:
python src/generate_data.py

3. –ó–∞–ø—É—Å—Ç–∏—Ç–µ ETL-–ø–∞–π–ø–ª–∞–π–Ω:
python src/etl_pipeline.py
## –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
–ü–æ—Å–ª–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ output/ –±—É–¥—É—Ç:
- –û—á–∏—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ñ–æ—Ä–º–∞—Ç–µ Parquet
- –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
- –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á–µ—Ç –æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏

### 7.3 –û—Ç—á–µ—Ç –≤ docs/report.md
```markdown
# –û—Ç—á–µ—Ç –ø–æ –ø—Ä–æ–µ–∫—Ç—É: ETL-–ø–∞–π–ø–ª–∞–π–Ω –Ω–∞ Apache Spark

## –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å—Ç—É–¥–µ–Ω—Ç–µ
- –§–ò–û: [–í–∞—à–µ –§–ò–û]
- –ì—Ä—É–ø–ø–∞: [–ù–æ–º–µ—Ä –≥—Ä—É–ø–ø—ã]
- –î–∞—Ç–∞: [–î–∞—Ç–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è]

## 1. –í—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ —à–∞–≥–∏

### 1.1 –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
- [x] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω JDK 11
- [x] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω Apache Spark 3.5.1
- [x] –ù–∞—Å—Ç—Ä–æ–µ–Ω—ã –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
- [x] –ü—Ä–æ–≤–µ—Ä–µ–Ω–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å

### 1.2 –ó–∞–ø—É—Å–∫ Spark –∫–ª–∞—Å—Ç–µ—Ä–∞
- [x] –ó–∞–ø—É—â–µ–Ω Master
- [x] –ó–∞–ø—É—â–µ–Ω Worker
- [x] –ü—Ä–æ–≤–µ—Ä–µ–Ω Web UI

### 1.3 –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞
- [x] –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
- [x] –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω ETL-–ø–∞–π–ø–ª–∞–π–Ω
- [x] –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
- [x] –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
- [x] –°–æ—Ö—Ä–∞–Ω–µ–Ω—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

## 2. –°–∫—Ä–∏–Ω—à–æ—Ç—ã

### 2.1 Spark Web UI
![Spark Web UI](screenshots/spark_ui.png)

### 2.2 –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞
![Terminal Output](screenshots/terminal_output.png)

### 2.3 –†–µ–∑—É–ª—å—Ç–∞—Ç—ã
![Results Folder](screenshots/results_folder.png)

## 3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö:
- –ò—Å—Ö–æ–¥–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: 55,000
- –ü–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏: [–≤–∞—à–µ —á–∏—Å–ª–æ]
- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: [–≤–∞—à–µ —á–∏—Å–ª–æ]
- –î–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç: [–¥–∏–∞–ø–∞–∑–æ–Ω]

### –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è:
- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö: [–≤—Ä–µ–º—è]
- ETL-–ø–∞–π–ø–ª–∞–π–Ω: [–≤—Ä–µ–º—è]
- –û–±—â–µ–µ –≤—Ä–µ–º—è: [–≤—Ä–µ–º—è]

## 4. –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

| –ü—Ä–æ–±–ª–µ–º–∞ | –†–µ—à–µ–Ω–∏–µ |
|----------|---------|
| [–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã] | [–ö–∞–∫ —Ä–µ—à–∏–ª–∏] |
| [–û–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã] | [–ö–∞–∫ —Ä–µ—à–∏–ª–∏] |

## 5. –í—ã–≤–æ–¥—ã
[–í–∞—à–∏ –≤—ã–≤–æ–¥—ã –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω–æ–π —Ä–∞–±–æ—Ç–µ]

### 7.4 –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã
1. Spark Web UI (docs/screenshots/spark_ui.png):
   - –ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ Master (localhost:8080)
   - –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å –∑–∞–ø—É—â–µ–Ω–Ω—ã–º Worker
   - –ò—Å—Ç–æ—Ä–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π

2. –¢–µ—Ä–º–∏–Ω–∞–ª (docs/screenshots/terminal_output.png):
   - –ó–∞–ø—É—Å–∫ Master –∏ Worker
   - –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ ETL-–ø–∞–π–ø–ª–∞–π–Ω–∞
   - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

3. –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (docs/screenshots/results_folder.png):
   - –°–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø–∞–ø–∫–∏ output/
   - –ü—Ä–∏–º–µ—Ä—ã —Ñ–∞–π–ª–æ–≤ Parquet –∏ CSV

4. –ö–æ–¥ –≤ VS Code (docs/screenshots/code.png):
   - –°–∫—Ä–∏–Ω—à–æ—Ç —Å –æ—Ç–∫—Ä—ã—Ç—ã–º–∏ —Ñ–∞–π–ª–∞–º–∏ –ø—Ä–æ–µ–∫—Ç–∞

### 7.5 –ß–µ–∫‚Äë–ª–∏—Å—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –∫ —Å–¥–∞—á–µ
- [ ] Spark Master –∏ Worker –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –±–µ–∑ –æ—à–∏–±–æ–∫
- [ ] generate_data.py —Å–æ–∑–¥–∞–µ—Ç data/clickstream.csv
- [ ] etl_pipeline.py –æ—Ç—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –±–µ–∑ –æ—à–∏–±–æ–∫ –∏ –ø–∏—à–µ—Ç –≤ output/
- [ ] verify_results.py –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
- [ ] –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
- [ ] README –∑–∞–ø–æ–ª–Ω–µ–Ω
- [ ] –û—Ç—á–µ—Ç –∏ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –¥–æ–±–∞–≤–ª–µ–Ω—ã

---

## 8. –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏

### –û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è (10 –±–∞–ª–ª–æ–≤):
1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (2 –±–∞–ª–ª–∞):
   - ‚úì Spark –∏ Java —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
   - ‚úì –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
   - ‚úì –ü—Ä–æ–≤–µ—Ä–æ—á–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã —Ä–∞–±–æ—Ç–∞—é—Ç

2. –ó–∞–ø—É—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞ (3 –±–∞–ª–ª–∞):
   - ‚úì Master –∑–∞–ø—É—â–µ–Ω
   - ‚úì Worker –ø–æ–¥–∫–ª—é—á–µ–Ω
   - ‚úì Web UI –¥–æ—Å—Ç—É–ø–µ–Ω
   - ‚úì –°–∫—Ä–∏–Ω—à–æ—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω—ã

3. ETL-–ø–∞–π–ø–ª–∞–π–Ω (3 –±–∞–ª–ª–∞):
   - ‚úì –î–∞–Ω–Ω—ã–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã
   - ‚úì ETL –≤—ã–ø–æ–ª–Ω–µ–Ω –±–µ–∑ –æ—à–∏–±–æ–∫
   - ‚úì –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
   - ‚úì –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞–µ—Ç

4. –û—Ç—á–µ—Ç –∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π (2 –±–∞–ª–ª–∞):
   - ‚úì –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ GitHub
   - ‚úì –ü–æ–ª–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞
   - ‚úì README.md –∑–∞–ø–æ–ª–Ω–µ–Ω
   - ‚úì –û—Ç—á–µ—Ç —Å —Å–∫—Ä–∏–Ω—à–æ—Ç–∞–º–∏

### –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –±–∞–ª–ª—ã (–¥–æ +5 –±–∞–ª–ª–æ–≤):
- –ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–¥–∞: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫, –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ (+1)
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ Spark, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (+1)
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∞–≥—Ä–µ–≥–∞—Ü–∏–π –∏–ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π (+1)
- –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –∫–æ–¥–µ, –æ–ø–∏—Å–∞–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã (+1)
- –ö—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç—å: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∏—á (–≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –¥–∞—à–±–æ—Ä–¥) (+1)

### –®—Ç—Ä–∞—Ñ—ã:
- -1 –±–∞–ª–ª –∑–∞ –∫–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é –ø—Ä–æ—Å—Ä–æ—á–∫–∏
- -2 –±–∞–ª–ª–∞ –∑–∞ –ø–ª–∞–≥–∏–∞—Ç (–∫–æ–¥ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–ø–∏—Å–∞–Ω —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ)
- -1 –±–∞–ª–ª –∑–∞ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Å–∫—Ä–∏–Ω—à–æ—Ç–æ–≤

---

## 9. –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ—Ç–∏:
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–æ—Ä—Ç–æ–≤
Test-NetConnection localhost -Port 8080
Test-NetConnection localhost -Port 7077

# –ü–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø–æ—Ä—Ç—ã
netstat -ano | findstr :8080

### –û—á–∏—Å—Ç–∫–∞ –ª–æ–≥–æ–≤:
# –û—á–∏—Å—Ç–∏—Ç—å –ª–æ–≥–∏ Spark
Remove-Item C:\spark\spark-3.5.1-bin-hadoop3\logs\* -Recurse -Force

# –û—á–∏—Å—Ç–∏—Ç—å –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
Remove-Item output\* -Recurse -Force
### –ü–µ—Ä–µ–∑–∞–ø—É—Å–∫ –∫–ª–∞—Å—Ç–µ—Ä–∞:
# –ó–∞–∫—Ä—ã—Ç—å –≤—Å–µ Java –ø—Ä–æ—Ü–µ—Å—Å—ã Spark
taskkill /F /IM java.exe

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, —á—Ç–æ –ø—Ä–æ—Ü–µ—Å—Å—ã –∑–∞–∫—Ä—ã—Ç—ã
tasklist | findstr java

---

## 10. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–±–æ—Ç–µ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏

### –ö–∞–∫ –∑–∞–¥–∞–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã:
1. –ö–æ–Ω–∫—Ä–µ—Ç–Ω–æ: "–ö–∞–∫ –∏—Å–ø—Ä–∞–≤–∏—Ç—å –æ—à–∏–±–∫—É Py4JNetworkError –ø—Ä–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–∏ –∫ Spark Master?"
2. –° –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º: –ü—Ä–∏–ª–æ–∂–∏—Ç–µ —Ç–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ –∏ —Å–≤–æ–π –∫–æ–¥
3. –ü–æ —à–∞–≥–∞–º: –†–∞–∑–±–µ–π—Ç–µ —Å–ª–æ–∂–Ω—É—é –ø—Ä–æ–±–ª–µ–º—É –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã

### –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
"–û–±—ä—è—Å–Ω–∏, —á—Ç–æ –¥–µ–ª–∞–µ—Ç —ç—Ç–∞ —Å—Ç—Ä–æ–∫–∞ –∫–æ–¥–∞ –≤ PySpark: 
df.groupBy('region').agg(F.count('*').alias('total'))"

"–ü–æ–º–æ–≥–∏ –Ω–∞–ø–∏—Å–∞—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫ –¥–ª—è Spark ETL –ø–∞–π–ø–ª–∞–π–Ω–∞"

"–ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å spark.executor.memory –¥–ª—è –¥–∞–Ω–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–º 1GB?"

### –ü–æ–ª–µ–∑–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:
- DeepSeek: –•–æ—Ä–æ—à –¥–ª—è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è –∫–æ–¥–∞
- GigaChat: –û—Ç–ª–∏—á–Ω–æ –æ–±—ä—è—Å–Ω—è–µ—Ç —Ç–µ–æ—Ä–∏—é –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- ChatGPT: –®–∏—Ä–æ–∫–∏–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏, –Ω–æ –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å —É—Ç–æ—á–Ω–µ–Ω–∏–π

---

## 11. –§–∏–Ω–∞–ª—å–Ω—ã–µ —à–∞–≥–∏

1. –°–æ–∑–¥–∞–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–∞ GitHub
2. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞
3. –í—ã–ø–æ–ª–Ω–∏—Ç–µ –∑–∞–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é
4. –°–¥–µ–ª–∞–π—Ç–µ —Å–∫—Ä–∏–Ω—à–æ—Ç—ã –≤—Å–µ—Ö —ç—Ç–∞–ø–æ–≤
5. –ù–∞–ø–∏—à–∏—Ç–µ –æ—Ç—á–µ—Ç –≤ docs/report.md
6. –û—Ç–ø—Ä–∞–≤—å—Ç–µ —Å—Å—ã–ª–∫—É –Ω–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—é

–°—Ä–æ–∫ —Å–¥–∞—á–∏: 2 –Ω–µ–¥–µ–ª–∏ —Å –º–æ–º–µ–Ω—Ç–∞ –≤—ã–¥–∞—á–∏ (–µ—Å–ª–∏ –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å –Ω–µ —É–∫–∞–∑–∞–ª –∏–Ω–∞—á–µ)

–í–∞–∂–Ω–æ: –ù–∞—á–∏–Ω–∞–π—Ç–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–¥–∞–Ω–∏—è –∑–∞—Ä–∞–Ω–µ–µ, —á—Ç–æ–±—ã –±—ã–ª–æ –≤—Ä–µ–º—è —Ä–µ—à–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã!

---

## 12. –ö–æ–Ω—Ç–∞–∫—Ç—ã –¥–ª—è –ø–æ–º–æ—â–∏
- –ü—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—å: [–ò–º—è –ø—Ä–µ–ø–æ–¥–∞–≤–∞—Ç–µ–ª—è, email]
- –ß–∞—Ç –∫—É—Ä—Å–∞: [–°—Å—ã–ª–∫–∞ –Ω–∞ Telegram/Discord]
- –ö–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏: [–î–Ω–∏ –∏ –≤—Ä–µ–º—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–π]

–£–¥–∞—á–∏ –≤ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞–Ω–∏—è! üöÄ